{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment-2-17110073-KavitaVaishnaw.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"gocZhGFUcIQw","colab_type":"code","outputId":"30254705-7a97-4c31-c21c-4485bddd201f","executionInfo":{"status":"ok","timestamp":1570383893865,"user_tz":-330,"elapsed":3143,"user":{"displayName":"Kavita Vaishnaw","photoUrl":"","userId":"03388939129326148581"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","datapath = '/content/grdrive/NLP Assignment/'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s5myYp-tcED0","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y8U8rmrUfMjh","colab_type":"code","outputId":"70797046-53b3-46a6-bd4a-52e33d1fde24","executionInfo":{"status":"ok","timestamp":1570383895922,"user_tz":-330,"elapsed":4573,"user":{"displayName":"Kavita Vaishnaw","photoUrl":"","userId":"03388939129326148581"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"TkscZ53FdADP","colab_type":"code","colab":{}},"source":["from nltk.tokenize import sent_tokenize"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SPS-RiYZUMPP","colab_type":"code","colab":{}},"source":["import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AHmvdV1e0kXr","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vpMgdCe6durR","colab_type":"code","colab":{}},"source":["full_text = open('/content/gdrive/My Drive/NLP Assignment/jane_austen.txt', 'r', errors='ignore').read()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rSYeX2C8in2v","colab_type":"code","colab":{}},"source":["full_text = full_text.replace('\\n', ' ').replace('\\r', ' ')\n","#.replace('*** START: FULL LICENSE ***', \" \")\n","#full_text = full_text.replace('SPEECH 1', ' ').replace('SPEECH 2', ' ').replace('SPEECH 3', ' ').replace('SPEECH 4', ' ').replace('SPEECH 5', ' ').replace('SPEECH 6', ' ').replace('SPEECH 7', ' ').replace('SPEECH 8', ' ').replace('SPEECH 9', ' ').replace('SPEECH 10', ' ')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JZAGfaWprcI6","colab_type":"code","colab":{}},"source":["import re"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FsCXJPIOhOjh","colab_type":"code","colab":{}},"source":["#full_text = re.sub(r'.*PERSUASION', '', full_text)\n","full_text = re.sub(r'Chapter\\s[0-9]+', '', full_text)\n","full_text = re.sub(r'THE END.*', '', full_text)\n","full_text = full_text.lower()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uOR_Gc1lVK6E","colab_type":"code","outputId":"fe2397c5-01f4-4bb1-b1ef-67dc813694e0","executionInfo":{"status":"ok","timestamp":1570383537639,"user_tz":-330,"elapsed":1987,"user":{"displayName":"Kavita Vaishnaw","photoUrl":"","userId":"03388939129326148581"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["print(\"CLASSICAL APPROACH...\")"],"execution_count":79,"outputs":[{"output_type":"stream","text":["CLASSICAL APPROACH...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aOCcYIQ7e5GG","colab_type":"code","colab":{}},"source":["sent_tokenize_list = sent_tokenize(full_text)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CTUfnvOKWxTv","colab_type":"code","colab":{}},"source":["for i in range(len(sent_tokenize_list)):\n","  sent_tokenize_list[i] = '# ' + sent_tokenize_list[i] + ' *'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TKZaABDm0lwK","colab_type":"code","colab":{}},"source":["train,test = train_test_split(sent_tokenize_list,test_size=0.2,random_state=40)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9pjhW1mqRm3k","colab_type":"code","colab":{}},"source":["tokens=[]\n","for i in range(len(train)):\n","    tokens = tokens + nltk.word_tokenize(train[i])\n","tokens_uniq = np.array(tokens)\n","tokens_uniq=np.unique(tokens_uniq)\n","types = list(tokens_uniq)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yv2st8ScR3Dz","colab_type":"code","outputId":"9c1af5c2-daba-4436-88c6-bef9167d323d","executionInfo":{"status":"ok","timestamp":1570383562553,"user_tz":-330,"elapsed":5107,"user":{"displayName":"Kavita Vaishnaw","photoUrl":"","userId":"03388939129326148581"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["print('Vocabulary Size: '+str(len(types)))"],"execution_count":84,"outputs":[{"output_type":"stream","text":["Vocabulary Size: 10599\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lJEMRNx8fWFu","colab_type":"code","colab":{}},"source":["from nltk.util import ngrams"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cw-UntYd45Kq","colab_type":"code","colab":{}},"source":["def extract_ngrams(data, num):\n","    n_grams = ngrams(nltk.word_tokenize(data), num)\n","    return [ ' '.join(grams) for grams in n_grams]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pHNZstqR5O_S","colab_type":"code","outputId":"981d1cb3-d088-438c-f9a5-aca1eeedc36b","executionInfo":{"status":"ok","timestamp":1570383565644,"user_tz":-330,"elapsed":5932,"user":{"displayName":"Kavita Vaishnaw","photoUrl":"","userId":"03388939129326148581"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["all_1grams = []\n","for sent in train:\n","  all_1grams += extract_ngrams(sent,1)\n","\n","print(\"Total possible unigrams: \"+ str(len(types)))\n","print(\"Number of unigrams that exist: \"+ str(len(np.unique(all_1grams))))"],"execution_count":87,"outputs":[{"output_type":"stream","text":["Total possible unigrams: 10599\n","Number of unigrams that exist: 10599\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3NkUJ8z5OOyo","colab_type":"code","outputId":"df6c6cfc-ef88-4a10-9431-45e5184f9f94","executionInfo":{"status":"ok","timestamp":1570383568946,"user_tz":-330,"elapsed":9020,"user":{"displayName":"Kavita Vaishnaw","photoUrl":"","userId":"03388939129326148581"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["all_2grams = []\n","for sent in train:\n","  all_2grams += extract_ngrams(sent,2)\n","print(\"Total possible bigrams: \"+ str(len(types)**2))\n","print(\"Number of bigrams that exist: \"+ str(len(np.unique(all_2grams))))"],"execution_count":88,"outputs":[{"output_type":"stream","text":["Total possible bigrams: 112338801\n","Number of bigrams that exist: 94691\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ys9kNwp9Puzo","colab_type":"code","outputId":"0cc1a8ff-15c8-4725-dca6-76bd39f8910a","executionInfo":{"status":"ok","timestamp":1570383572191,"user_tz":-330,"elapsed":12073,"user":{"displayName":"Kavita Vaishnaw","photoUrl":"","userId":"03388939129326148581"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["all_3grams = []\n","for sent in train:\n","  all_3grams += extract_ngrams(sent,3)\n","print(\"Total possible trigrams: \"+ str(len(types)**3))\n","print(\"Number of trigrams that exist: \"+ str(len(np.unique(all_3grams))))"],"execution_count":89,"outputs":[{"output_type":"stream","text":["Total possible trigrams: 1190678951799\n","Number of trigrams that exist: 208528\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"db0a0tuNQkzx","colab_type":"code","outputId":"ba63107b-a514-403a-8d9b-9c1af14ea416","executionInfo":{"status":"ok","timestamp":1570383575516,"user_tz":-330,"elapsed":15235,"user":{"displayName":"Kavita Vaishnaw","photoUrl":"","userId":"03388939129326148581"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["all_4grams = []\n","for sent in train:\n","  all_4grams += extract_ngrams(sent,4)\n","print(\"Total possible quadgrams: \"+ str(len(types)**4))\n","print(\"Number of quadgrams that exist: \"+ str(len(np.unique(all_4grams))))"],"execution_count":90,"outputs":[{"output_type":"stream","text":["Total possible quadgrams: 12620006210117601\n","Number of quadgrams that exist: 263670\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"26fN_IUvVnC_","colab_type":"code","colab":{}},"source":["count_table_1gram = {}\n","for word in all_1grams:\n","  if word in count_table_1gram:\n","    count_table_1gram[word] += 1\n","  else:\n","    count_table_1gram[word] = 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"00PVz3iPWN5s","colab_type":"code","colab":{}},"source":["count_table_2gram = {}\n","for word in all_2grams:\n","  word_list = word.split(\" \")\n","  context = \" \".join(word_list[0:-1])\n","  if context in count_table_2gram: \n","    if word_list[-1] in count_table_2gram[context]:\n","      count_table_2gram[context][word_list[-1]] += 1\n","    else:\n","      count_table_2gram[context][word_list[-1]] = 1\n","  else:\n","    count_table_2gram[context]={}\n","    count_table_2gram[context][word_list[-1]] = 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ftI7APLPYl45","colab_type":"code","colab":{}},"source":["count_table_3gram = {}\n","for word in all_3grams:\n","  word_list = word.split(\" \")\n","  context = \" \".join(word_list[0:-1])\n","  if context in count_table_3gram: \n","    if word_list[-1] in count_table_3gram[context]:\n","      count_table_3gram[context][word_list[-1]] += 1\n","    else:\n","      count_table_3gram[context][word_list[-1]] = 1\n","  else:\n","    count_table_3gram[context]={}\n","    count_table_3gram[context][word_list[-1]] = 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4_VdDtL0Y20Z","colab_type":"code","colab":{}},"source":["count_table_4gram = {}\n","for word in all_4grams:\n","  word_list = word.split(\" \")\n","  context = \" \".join(word_list[0:-1])\n","  if context in count_table_4gram: \n","    if word_list[-1] in count_table_4gram[context]:\n","      count_table_4gram[context][word_list[-1]] += 1\n","    else:\n","      count_table_4gram[context][word_list[-1]] = 1\n","  else:\n","    count_table_4gram[context]={}\n","    count_table_4gram[context][word_list[-1]] = 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"klCgmLRXN4Su","colab_type":"code","colab":{}},"source":["def MLE(count_table, context, word):\n","  try:\n","    if context == '':\n","      return float(count_table[word]/sum(count_table.values()))\n","    else:\n","      count_ngram = count_table[context][word]\n","      count_context = sum(count_table[context].values())\n","      return float(count_ngram/count_context)\n","  except:\n","    return 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fGkpl90peOZm","colab_type":"code","colab":{}},"source":["count_tables = [count_table_1gram, count_table_2gram, count_table_3gram, count_table_4gram]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AcqzhTAqZYBx","colab_type":"code","colab":{}},"source":["def Generator(num, sent_count):\n","  for i in range(sent_count):\n","    new_sent = [\"#\"]    \n","    while (len(new_sent) < num-1):\n","      chosen = '' \n","      random = np.random.multinomial(1, [MLE(count_tables[1], \" \".join(new_sent[-1:]), word) for word in np.unique(all_1grams)], size=1)\n","      for i in range(len(random[0])):\n","        if random[0][i] == 1:\n","          chosen = np.unique(all_1grams)[i]\n","      new_sent.append(chosen)\n","    chosen = ''\n","    words_in_sent = 0  \n","    while (chosen!='*' and words_in_sent<25):\n","      if num==1:\n","        random = np.random.multinomial(1, [MLE(count_tables[0], \"\", word) for word in np.unique(all_1grams)], size=1)\n","      else:\n","        random = np.random.multinomial(1, [MLE(count_tables[num-1], \" \".join(new_sent[-(num-1):]), word) for word in np.unique(all_1grams)], size=1)\n","      for i in range(len(random[0])):\n","        if random[0][i] == 1:\n","          chosen = np.unique(all_1grams)[i]\n","      new_sent.append(chosen)\n","      words_in_sent += 1\n","    print(\" \".join(new_sent[1:-1]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AxC1Zj1FE9rI","colab_type":"code","colab":{}},"source":["test_tokens=[]\n","for i in range(len(test)):\n","    test_tokens = test_tokens + nltk.word_tokenize(test[i])\n","#tokens_uniq = np.array(tokens)\n","#tokens_uniq=np.unique(tokens_uniq)\n","#types = list(tokens_uniq)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ld0NiTDwUbd-","colab_type":"code","colab":{}},"source":["from statistics import mean"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-FUOohZFFm33","colab_type":"code","colab":{}},"source":["def prob_calc(sent_list, num):\n","  prob = [];\n","  for i in range(len(sent_list)):\n","    words_in_sent = nltk.word_tokenize(sent_list[i])\n","    for k in range(len(words_in_sent)):\n","      if num == 1:\n","        mle = MLE(count_tables[0], \"\", words_in_sent[k])\n","      else:\n","        if k < num-1:\n","          mle = MLE(count_tables[0], \"\", words_in_sent[k])\n","        else: \n","          mle = MLE(count_tables[num-1], \" \".join(words_in_sent[k-(num-1):k]), word)\n","      if mle != 0:\n","        prob.append(np.log(mle))\n","  return np.exp(mean(prob))\n","  #return np.exp(prob)\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BOsvabzeRdiU","colab_type":"code","colab":{}},"source":["def pplx_calc(sent_list, num):\n","  prob = prob_calc(sent_list, num)\n","  pplx = prob ** (-1/len(test_tokens))\n","  return pplx"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cVEtBM5iSv4r","colab_type":"code","outputId":"54c45de0-322f-4e06-8262-cb1fa9b36cb8","executionInfo":{"status":"ok","timestamp":1570383591689,"user_tz":-330,"elapsed":20406,"user":{"displayName":"Kavita Vaishnaw","photoUrl":"","userId":"03388939129326148581"}},"colab":{"base_uri":"https://localhost:8080/","height":117}},"source":["print(\"Perplexity of the different Language Models on the test data: \\n\")\n","print(\"Unigram Model: \" + str(pplx_calc(test,1)))\n","print(\"Bigram Model: \" + str(pplx_calc(test,2)))\n","print(\"Trigram Model: \" + str(pplx_calc(test,3)))\n","print(\"Quadgram Model: \" + str(pplx_calc(test,4)))"],"execution_count":102,"outputs":[{"output_type":"stream","text":["Perplexity of the different Language Models on the test data: \n","\n","Unigram Model: 1.0000729882878776\n","Bigram Model: 1.0000412541697576\n","Trigram Model: 1.0000545737006532\n","Quadgram Model: 1.0000629581935199\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dTS3G6Zrfgpd","colab_type":"code","outputId":"bb048727-8ef0-4ad0-a88d-700793820366","executionInfo":{"status":"ok","timestamp":1570383681450,"user_tz":-330,"elapsed":105625,"user":{"displayName":"Kavita Vaishnaw","photoUrl":"","userId":"03388939129326148581"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["print(\"Sentences Generated by Unigram LM: \\n\")\n","Generator(1, 5)\n","print(\"\\n\\nComments: Most of the sentences are grammatically incorrect and do not make any sense because they are just series of words chosen at random. Also, the sentences are intersparsed with random occurences of punctuation marks because the model is not taking context into account.\")"],"execution_count":103,"outputs":[{"output_type":"stream","text":["Sentences Generated by Unigram LM: \n","\n","entirely does joined than the of , distressing was support '' well out # by to had he using get\n","his , few itself learnt to\n","morland if observe it and be modern average `` #\n","not amongst . fears to of this to serve # not morland norris consolation `` his returned steady a so i of the dinner\n","commodious with feelings west , bertrams ,\n","\n","\n","Comments: Most of the sentences are grammatically incorrect and do not make any sense because they are just series of words chosen at random. Also, the sentences are intersparsed with random occurences of punctuation marks because the model is not taking context into account.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0LWZKH0jjmhx","colab_type":"code","outputId":"9852aa92-8dc2-4724-9e00-ebea85b59981","executionInfo":{"status":"ok","timestamp":1570383722671,"user_tz":-330,"elapsed":28304,"user":{"displayName":"Kavita Vaishnaw","photoUrl":"","userId":"03388939129326148581"}},"colab":{"base_uri":"https://localhost:8080/","height":184}},"source":["print(\"Sentences Generated by Bigram LM:\\n\")\n","Generator(2,5)\n","print(\"\\n\\nComments: Although the sentences do not have random occurences of punctuation marks like in the case of unigram model, the sentences still do not make much sense.\")"],"execution_count":104,"outputs":[{"output_type":"stream","text":["Sentences Generated by Bigram LM:\n","\n","his disorder increased pain , and used to her , a repetition ; and that room for out of tenderness toward herself in the\n","she talked a future war with a frigate , though she deserved ; and even when louisa and with perfect model of such a\n","where a home at school while anne could , so little drawing-room , and do i. we should allow myself ; and i had\n","we respect .\n","captain wentworth , perhaps i am not rather have passed between them into the evening , dear !\n","\n","\n","Comments: Although the sentences do not have random occurences of punctuation marks like in the case of unigram model, the sentences still do not make much sense.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YxiDbs_IjpG-","colab_type":"code","outputId":"b32c7a17-81e5-493e-94f5-bcbdb5b6b08b","executionInfo":{"status":"ok","timestamp":1570383756520,"user_tz":-330,"elapsed":28130,"user":{"displayName":"Kavita Vaishnaw","photoUrl":"","userId":"03388939129326148581"}},"colab":{"base_uri":"https://localhost:8080/","height":184}},"source":["print(\"Sentences Generated by Trigram LM:\\n\")\n","Generator(3,5)\n","print(\"\\n\\nComments: Some parts of the sentences make more sense because the model is capturing more context than before.\")"],"execution_count":105,"outputs":[{"output_type":"stream","text":["Sentences Generated by Trigram LM:\n","\n","but they could not forbear , in a state of utter barbarism , to interfere with the strafford family .\n","perhaps they ought to have plenty of seeing them .\n","sometimes it appeared that blaize castle remained her only concern of higher interest .\n","she was in the garden with mackenzie , trying to bury some of the hand ; but the evil would have expected anything rather than\n","she was sure they would be turned , repeating her conviction of his coming might have warned her , which fills up the room ,\n","\n","\n","Comments: Some parts of the sentences make more sense because the model is capturing more context than before.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KuuRCua9jtMY","colab_type":"code","outputId":"cf761a4e-c8bc-43a6-cb23-f443309b4bc1","executionInfo":{"status":"ok","timestamp":1570383822803,"user_tz":-330,"elapsed":27068,"user":{"displayName":"Kavita Vaishnaw","photoUrl":"","userId":"03388939129326148581"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["print(\"Sentences Generated by Quadgram LM:\\n\")\n","Generator(4,5)\n","print(\"\\n\\nComments: The sentences generated are very similar to the sentences in the training set because a lot of the context is being considered.\")"],"execution_count":106,"outputs":[{"output_type":"stream","text":["Sentences Generated by Quadgram LM:\n","\n","a few days , he could not have been introduced to each other with a considerable portion of apparent indifference and calmness ; but the only\n","there indulged zealous zealous zealous zealous zealous zealous zealous zealous zealous zealous zealous zealous zealous zealous zealous zealous zealous zealous zealous zealous zealous zealous zealous zealous\n","i hope she does not do what her aunt and to herself .\n","i would not go for the sake of being travelling at the same time from susan 's .\n","i did not feel equal to it .\n","\n","\n","Comments: The sentences generated are very similar to the sentences in the training set because a lot of the context is being considered.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vl25POtHYXjP","colab_type":"code","outputId":"95888dfd-5bae-44f7-9a0e-1290c2b2efe3","executionInfo":{"status":"ok","timestamp":1570383845358,"user_tz":-330,"elapsed":1279,"user":{"displayName":"Kavita Vaishnaw","photoUrl":"","userId":"03388939129326148581"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["print(\"NEURAL APPROACH...\")"],"execution_count":107,"outputs":[{"output_type":"stream","text":["NEURAL APPROACH...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0C0kPFQRVGy-","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import SimpleRNN\n","from keras.layers import Embedding\n","from keras.layers import RNN\n","from keras.utils import np_utils\n","import keras.preprocessing.text as ktp"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qLwAPNGGdBB0","colab_type":"code","colab":{}},"source":["words_list = list(ktp.text_to_word_sequence(full_text, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'', lower=True, split=' '))\n","words = sorted(list(set(words_list)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iphcpJAGfKJw","colab_type":"code","colab":{}},"source":["n_to_word = {n:word for n, word in enumerate(words)}\n","word_to_n = {word:n for n, word in enumerate(words)}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0KbUY-9niTqV","colab_type":"code","outputId":"c5ad80ea-a254-473d-b307-dd146dfea5d7","executionInfo":{"status":"ok","timestamp":1570383961761,"user_tz":-330,"elapsed":1270,"user":{"displayName":"Kavita Vaishnaw","photoUrl":"","userId":"03388939129326148581"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["vocab_size = len(words)\n","print('Number of unique words: ', vocab_size)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Number of unique words:  10811\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qqqVLW8pikoI","colab_type":"code","colab":{}},"source":["X = []   # extracted sequences ---- train array\n","Y = []   # follow up word for each sequence in X ---- target array\n","length = len(words_list)\n","seq_length = 7\n","\n","for i in range(0, length - seq_length, 1):\n","    sequence = words_list[i:i + seq_length]\n","    label = words_list[i + seq_length]\n","    X.append([word_to_n[w] for w in sequence])\n","    Y.append(word_to_n[label])\n","    \n","#print('Number of extracted sequences:', len(X))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"viMiVPZlpTQG","colab_type":"code","colab":{}},"source":["X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i0o-R3V7kAmq","colab_type":"code","colab":{}},"source":["X_mod = np.reshape(X_train, (len(X_train), seq_length, 1))\n","X_mod = X_mod / float(len(words))\n","Y_mod = np_utils.to_categorical(Y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e5TuMvZ5l38H","colab_type":"code","outputId":"1da2c47f-3e14-46de-c77a-6a5d31f762ec","executionInfo":{"status":"ok","timestamp":1570384003516,"user_tz":-330,"elapsed":950,"user":{"displayName":"Kavita Vaishnaw","photoUrl":"","userId":"03388939129326148581"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["model = Sequential()\n","#model.add(Embedding(vocab_size, 100))\n","model.add(SimpleRNN(100, return_sequences = True))\n","#, input_shape=(X_mod.shape[1], X_mod.shape[2]), return_sequences=True\n","model.add(Dropout(0.2))\n","model.add(SimpleRNN(100))\n","model.add(Dropout(0.2))\n","model.add(Dense(len(words), activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":19,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a6nkR1Jt_MGo","colab_type":"code","colab":{}},"source":["from keras.callbacks import ReduceLROnPlateau\n","reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n","                              patience=1, min_lr=0.001)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TZbdaMqrnQ4Y","colab_type":"code","outputId":"e646d4ca-3918-4768-9311-93dbc976d777","executionInfo":{"status":"ok","timestamp":1570385250978,"user_tz":-330,"elapsed":546937,"user":{"displayName":"Kavita Vaishnaw","photoUrl":"","userId":"03388939129326148581"}},"colab":{"base_uri":"https://localhost:8080/","height":368}},"source":["model.fit(X_mod, Y_mod, epochs=10, batch_size=128, callbacks = [reduce_lr])"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","259205/259205 [==============================] - 54s 209us/step - loss: 6.2028 - acc: 0.0422\n","Epoch 2/10\n","259205/259205 [==============================] - 54s 209us/step - loss: 6.1853 - acc: 0.0424\n","Epoch 3/10\n","259205/259205 [==============================] - 54s 209us/step - loss: 6.1671 - acc: 0.0427\n","Epoch 4/10\n","259205/259205 [==============================] - 54s 210us/step - loss: 6.1544 - acc: 0.0423\n","Epoch 5/10\n","259205/259205 [==============================] - 55s 210us/step - loss: 6.1393 - acc: 0.0427\n","Epoch 6/10\n","259205/259205 [==============================] - 55s 213us/step - loss: 6.1234 - acc: 0.0424\n","Epoch 7/10\n","259205/259205 [==============================] - 55s 211us/step - loss: 6.1126 - acc: 0.0425\n","Epoch 8/10\n","259205/259205 [==============================] - 55s 211us/step - loss: 6.0977 - acc: 0.0419\n","Epoch 9/10\n","259205/259205 [==============================] - 55s 213us/step - loss: 6.0854 - acc: 0.0425\n","Epoch 10/10\n","259205/259205 [==============================] - 55s 211us/step - loss: 6.0726 - acc: 0.0424\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fe84fa81b00>"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"6cM4U915VpLp","colab_type":"code","colab":{}},"source":["X_modtest = np.reshape(X_test, (len(X_test), seq_length, 1))\n","X_modtest = X_modtest / float(len(words))\n","Y_modtest = np_utils.to_categorical(Y_test, num_classes = 10811)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4iP39J7y_yr_","colab_type":"code","colab":{}},"source":["def sample(preds, temperature=1.0):\n","  # helper function to sample an index from a probability array\n","  preds\n","  preds = np.asarray(preds).astype('float64')\n","  preds = np.log(preds) / temperature\n","  exp_preds = np.exp(preds)\n","  preds = exp_preds / np.sum(exp_preds)\n","  #print(preds[0,:])\n","  probas = np.random.multinomial(1, preds, 1)\n","  probas\n","  return np.argmax(probas)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jc6c-RIz5OQj","colab_type":"code","colab":{}},"source":["def make_a_sent(model):\n","  start = np.random.randint(0, len(X)-1) # or generate random start\n","  words_mapped = list(X[start])\n","  sent = [n_to_word[value] for value in X[start]]\n","  # generating words\n","  started = 1\n","  pred_index = 0\n","  for i in range(20):\n","    x = np.reshape(words_mapped,(1,len(words_mapped), 1))\n","    x = x / float(len(words))\n","    pred_index = sample(model.predict(x, verbose=0)[0,:])\n","    #print(words_mapped)\n","    sent.append(n_to_word[pred_index])\n","\n","    words_mapped.append(pred_index)  # add the predicted word to the end\n","    words_mapped = words_mapped[1:len(words_mapped)] # shift the string one word forward by removing pos. 0\n","  output = \" \".join(sent)\n","  return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o1IcKR9zMzuH","colab_type":"code","outputId":"c9bf8966-21d8-419a-a77f-6741bf168ff2","executionInfo":{"status":"ok","timestamp":1570385254392,"user_tz":-330,"elapsed":3348,"user":{"displayName":"Kavita Vaishnaw","photoUrl":"","userId":"03388939129326148581"}},"colab":{"base_uri":"https://localhost:8080/","height":184}},"source":["print(\"Sentences Generated by Vanilla RNN:\\n\")\n","print(make_a_sent(model))\n","print(make_a_sent(model))\n","print(make_a_sent(model))\n","print(make_a_sent(model))\n","print(make_a_sent(model))\n","print(\"\\n\\nComments: The sentences do not make sense. They are not readable.\")"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Sentences Generated by Vanilla RNN:\n","\n","go her cousin was safe on the at first a passed with creature sensible rest which at to man highly wonder time she not anything till first\n","his sister and whispered my dear creature entered its her he own if that to came pleasure something she sight being least not seen looked its for\n","of some grandeur in any particular style he common them it yourself them it a doubt fall sort probably you ragged i dear of down agreeable i\n","been governed by motives of selfishness and one cash the give if still were she persuade the you reigned catherine them matter week mischief done we house\n","say that he believed her to have she with will when longed talked you undoubtedly favourable a having he had you making of it that learn and\n","\n","\n","Comments: The sentences do not make sense. They are not readable.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TQ_XEVQhN8pE","colab_type":"code","colab":{}},"source":["from keras.layers import LSTM"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BgHrUSVHNS_f","colab_type":"code","colab":{}},"source":["model_lstm = Sequential()\n","#model.add(Embedding(vocab_size, 100))\n","model_lstm.add(LSTM(100, input_shape=(X_mod.shape[1], X_mod.shape[2]), return_sequences=True))\n","#\n","model_lstm.add(Dropout(0.2))\n","model_lstm.add(LSTM(100))\n","model_lstm.add(Dropout(0.2))\n","model_lstm.add(Dense(Y_mod.shape[1], activation='softmax'))\n","model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GRF2zgbTOCuH","colab_type":"code","outputId":"dfceda10-cc84-427b-c1b1-23a2edf8c882","executionInfo":{"status":"ok","timestamp":1570386290245,"user_tz":-330,"elapsed":1039177,"user":{"displayName":"Kavita Vaishnaw","photoUrl":"","userId":"03388939129326148581"}},"colab":{"base_uri":"https://localhost:8080/","height":368}},"source":["model_lstm.fit(X_mod, Y_mod, epochs=10, batch_size=128, callbacks = [reduce_lr])"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","259205/259205 [==============================] - 107s 414us/step - loss: 6.5969 - acc: 0.0369\n","Epoch 2/10\n","259205/259205 [==============================] - 104s 401us/step - loss: 6.4554 - acc: 0.0384\n","Epoch 3/10\n","259205/259205 [==============================] - 103s 398us/step - loss: 6.4404 - acc: 0.0387\n","Epoch 4/10\n","259205/259205 [==============================] - 103s 397us/step - loss: 6.4322 - acc: 0.0388\n","Epoch 5/10\n","259205/259205 [==============================] - 102s 395us/step - loss: 6.4282 - acc: 0.0391\n","Epoch 6/10\n","259205/259205 [==============================] - 103s 397us/step - loss: 6.4243 - acc: 0.0390\n","Epoch 7/10\n","259205/259205 [==============================] - 103s 397us/step - loss: 6.4215 - acc: 0.0391\n","Epoch 8/10\n","259205/259205 [==============================] - 103s 396us/step - loss: 6.4178 - acc: 0.0390\n","Epoch 9/10\n","259205/259205 [==============================] - 103s 396us/step - loss: 6.4143 - acc: 0.0392\n","Epoch 10/10\n","259205/259205 [==============================] - 103s 397us/step - loss: 6.4099 - acc: 0.0392\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fe84f4aee10>"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"p0JtX4tnRL8o","colab_type":"code","outputId":"5ff59e99-fdfc-4993-e428-71c8080c2bc5","executionInfo":{"status":"ok","timestamp":1570386292122,"user_tz":-330,"elapsed":1041045,"user":{"displayName":"Kavita Vaishnaw","photoUrl":"","userId":"03388939129326148581"}},"colab":{"base_uri":"https://localhost:8080/","height":184}},"source":["print(\"Sentences Generated by RNN with LSTM:\\n\")\n","print(make_a_sent(model_lstm))\n","print(make_a_sent(model_lstm))\n","print(make_a_sent(model_lstm))\n","print(make_a_sent(model_lstm))\n","print(make_a_sent(model_lstm))\n","print(\"\\n\\nComments: The sentences are not readable, that is, do not make sense. But they contain some phrases (group of words) that are meaningful.\")"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Sentences Generated by RNN with LSTM:\n","\n","you very particularly asked even if you only her will in that and as the any you must she has my rejoice us as anxiety locked that\n","a scheme among such a party and that brought proudly and of together and come part making with going time left in visited a but many it\n","determined they should not say i shut do but their did him spoke of been of build does his down in manner do nothing s way admit\n","them and felt for them with a a he suffer been you i young especially upon to hand joined husband nothing preferred in i amazed resumed her\n","enclosing two more one from the secretary and to her mansfield of as is judgment heard plans that not had among without and through morland the wind\n","\n","\n","Comments: The sentences are not readable, that is, do not make sense. But they contain some phrases (group of words) that are meaningful.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lq75dE-k5cVH","colab_type":"code","colab":{}},"source":["def prob_sent(X_test, Y_test, model):\n","  tot_prob = []\n","  i=0\n","  while (i<len(X_test)):\n","    prob_sent = 0\n","    for j in range(20):\n","      try:\n","        words_mapped = list(X_test[i])\n","        x = np.reshape(words_mapped,(1,len(words_mapped), 1))\n","        x = x / float(len(words))\n","        y = model.predict(x,verbose=0)[0,:]\n","        pred = np.argmax(Y_test[i])\n","        #print(y[pred])\n","        if y[pred]!=0:\n","          prob = np.log(y[pred])\n","          prob_sent += prob\n","        i+=1\n","      except:\n","        continue\n","    tot_prob.append(np.exp(prob_sent))\n","  return np.mean(tot_prob)\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yJb56oYc9JCB","colab_type":"code","colab":{}},"source":["def ppx(X_test, Y_test, model):\n","  prob = prob_sent(X_test, Y_test, model)\n","  N = len(Y_test)\n","  return prob**(-1/N)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t0UhQql972yl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"ea5fb029-a5d8-4410-acaf-be10c5a2ace4"},"source":["print(\"Perplexity of different neural models on the test data set:\\n\")\n","print(\"Vanilla RNN: \"+ str(ppx(X_modtest,Y_modtest, model)))\n","print(\"LSTM: \"+ str(ppx(X_modtest, Y_modtest, model_lstm)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Perplexity of different neural models on the test data set:\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7GXAWaEeXZOI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"9cb34d80-5cf2-4c6f-c70c-e886d6421d36","executionInfo":{"status":"ok","timestamp":1570386293598,"user_tz":-330,"elapsed":1458,"user":{"displayName":"Kavita Vaishnaw","photoUrl":"","userId":"03388939129326148581"}}},"source":["print(\"The text generated by n-gram model, especially in case of trigrams and quadgrams is more readable than the neural models. Neural models attemot to capture more context during training and prediction. However, at 10 epochs, they are not able to make good predictions. They could perform better with more number of epochs. I could not test that due to time constraint. However, too many epochs can also lead to overfitting. N-gram models are taking a smaller context into consideration but they are choosing from actual n-gram occurences. So, they are making more meaningful predictions in this case of text generation.\")"],"execution_count":42,"outputs":[{"output_type":"stream","text":["The text generated by n-gram model, especially in case of trigrams and quadgrams is more readable than the neural models. Neural models attemot to capture more context during training and prediction. However, at 10 epochs, they are not able to make good predictions. They could perform better with more number of epochs. I could not test that due to time constraint. However, too many epochs can also lead to overfitting. N-gram models are taking a smaller context into consideration but they are choosing from actual n-gram occurences. So, they are making more meaningful predictions in this case of text generation.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jzC2gvDqS4Ur","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}